[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website shows work I did in DS 002R: Foundations of Data Science in R, at Pomona College in spring 2024."
  },
  {
    "objectID": "EnglishEd.html",
    "href": "EnglishEd.html",
    "title": "Education in English Towns",
    "section": "",
    "text": "This is analysis of a data set of education in English towns, which is from the 2021 cencus by The UK Office for National Statistics. The data can be found at https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/educationandchildcare/datasets/educationalattainmentofyoungpeopleinenglishtownsdata/200708201819/youngpeoplesattainmentintownsreferencetable1.xlsx.\n\nlibrary(ggplot2)\nenglish_education &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-23/english_education.csv')\n\nggplot(english_education, aes(y = population_2011 / 1000, x = education_score)) +\n  geom_point(aes(color = university_flag),  alpha = 0.5) +\n  scale_y_log10() +\n  labs(\n    y = \"town population (1000s)\",\n    x = \"education score\",\n    title = \"Town Population in English Towns Vs. Education Score\"\n)\n\n\n\n\nAnalyzing this plot, the first think I noticed is that the towns with universities tend to be larger than those without, with all the largest towns having universities. It also seemed that having a university did not significantly alter the expected education scores. More generally, I noticed that larger towns tended to skew towards lower education scores."
  },
  {
    "objectID": "Slides.html",
    "href": "Slides.html",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "",
    "text": "This is analysis of a data set of all of Emily Dickinson’s poems, which can be found at https://www.gutenberg.org/files/12242/12242-h/12242-h.htm. I first decided to organize the data as a long data frame. Each unit of observation is a poem. The table includes three columns, the series, ID, and text of the poem for each unit of observation."
  },
  {
    "objectID": "Slides.html#the-data",
    "href": "Slides.html#the-data",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "",
    "text": "This is analysis of a data set of all of Emily Dickinson’s poems, which can be found at https://www.gutenberg.org/files/12242/12242-h/12242-h.htm. I first decided to organize the data as a long data frame. Each unit of observation is a poem. The table includes three columns, the series, ID, and text of the poem for each unit of observation."
  },
  {
    "objectID": "Slides.html#trying-to-analyze-it",
    "href": "Slides.html#trying-to-analyze-it",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "Trying to Analyze It",
    "text": "Trying to Analyze It\nI made some plots investigating the number of lines in each poem, but they weren’t very satisfying."
  },
  {
    "objectID": "Slides.html#problem-with-plotting-words",
    "href": "Slides.html#problem-with-plotting-words",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "Problem with Plotting Words",
    "text": "Problem with Plotting Words\nHow do you make a plot that visualizes text?\n\nBrowsing for Inspiration\n\nLooking at examples of different kinds of plots"
  },
  {
    "objectID": "Slides.html#wordcloud2",
    "href": "Slides.html#wordcloud2",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "Wordcloud2",
    "text": "Wordcloud2\nwordcloud2() from the wordcloud2 package allows me to create a word cloud from a data frame where a unit of observation is a word and the number of times it appears in the text.\n\nword_freq &lt;- data.frame(text = str_c(poems$poem)) |&gt; \n  mutate(text = tolower(text)) |&gt; \n  mutate(text = str_remove_all(text, '[[:punct:]]|\\\\|')) |&gt; \n  mutate(tokens = str_split(text, \"\\\\s+\")) |&gt;\n  unnest(cols = c(tokens)) |&gt; \n  count(tokens) |&gt;\n  arrange(desc(n)) |&gt; \n  rename(word = tokens, freq = n)\nwordcloud2(data = word_freq, size = 2.5)"
  },
  {
    "objectID": "Slides.html#ugly-plot",
    "href": "Slides.html#ugly-plot",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "Ugly Plot",
    "text": "Ugly Plot"
  },
  {
    "objectID": "Slides.html#spacy-spacyr",
    "href": "Slides.html#spacy-spacyr",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "Spacy & Spacyr",
    "text": "Spacy & Spacyr\nSpacy is a powerful python based Natural Language Processing package. Spacyr is an R wrapper for the spacy package."
  },
  {
    "objectID": "Slides.html#using-spacyr",
    "href": "Slides.html#using-spacyr",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "Using Spacyr",
    "text": "Using Spacyr\nI don’t know how to use spacyr, so I borrowed this chunk of code from a generous user on stack exchange.\n\nspacy_install()\nspacy_initialize(model = \"en_core_web_sm\")\nnouns &lt;- unique((str_c(poems$poem) |&gt; tolower() |&gt;\n            spacy_parse(pos = TRUE) |&gt; unnest_tokens(word, token) |&gt;\n              filter(pos == \"NOUN\"))[[\"word\"]])\nexcluded_words &lt;- c(\"t\", \"so\", \"by\", \"be\", \"go\", \"tell\")\n\nIt provides a list of all the nouns in a string of text. I fed it a big combined string of all the text from my poem column. I also included a secondary list of words that spacyr missed but I did not consider nouns."
  },
  {
    "objectID": "Slides.html#pretty-plot",
    "href": "Slides.html#pretty-plot",
    "title": "How I made a word cloud of Emily Dickinson’s Poems",
    "section": "Pretty Plot",
    "text": "Pretty Plot\nAfter filtering ‘word_freq’ for words in my noun list and not in my excluded words list, I was left with a clean word cloud."
  },
  {
    "objectID": "MeanAbsorbance.html",
    "href": "MeanAbsorbance.html",
    "title": "Mean Absorbance Investigation",
    "section": "",
    "text": "Here I recreate figure 1 “Mean Absorbance from Each Publication in the WAI Database” from Voss (2020) using the data from Wideband Acoustic Immittance (WAI) Database hosted by Smith College.\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname= \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nThe following SQL query calculates the average absorbance for a series of acoustic or measurement data grouped by frequency, identifier, and instrument. It filters for data associated with specific studies (indicated by their identifiers) and joins this with the study metadata, including author details. Results are grouped and sorted by frequency, identifier, and instrument.\n\n    SELECT \n        m.Frequency,\n        m.Identifier,\n        m.Instrument,\n        AVG(m.Absorbance) AS mean_absorbance, \n        CONCAT(pi.AuthorsShortList, ' (', pi.Year, ')', ' N=', COUNT(DISTINCT CONCAT(m.SubjectNumber, '-', m.Ear)),'; ', m.Instrument) AS Label\n    FROM \n        Measurements m\n    JOIN \n        PI_Info pi\n        ON m.Identifier = pi.Identifier\n    WHERE \n        m.Identifier IN ('Abur_2014', 'Feeney_2017', 'Groon_2015', 'Lewis_2015', 'Lui_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\n    GROUP BY \n        m.Frequency, \n        m.Identifier, \n        m.Instrument, \n        pi.Year, \n        pi.AuthorsShortList\n    ORDER BY \n        m.Frequency, \n        m.Identifier, \n        m.Instrument\n\nThis R code creates a plot of mean absorbance from each study.\n\nggplot(new_table, aes(x = Frequency, y = mean_absorbance, color = Label)) +\n  geom_line(size = 0.5, na.rm = TRUE) +  \n  scale_x_continuous(\n    name = \"Frequency (Hz)\",\n    breaks = c(200, 1000, 2000, 4000, 8000),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    name = \"Mean Absorbance\",\n    limits = c(0, 1), \n    breaks = seq(0, 1, by = 0.1)  \n  ) +\n  labs(\n    title = \"Mean Absorbance From Each Publication in the WAI Database\",  \n    color = NULL \n  ) +\n  theme_minimal()\n\n\n\n\nAnalyzing the plot, I see that mean absorbance sharply increases up to around 1000Hz and remains high before decreasing after 4000Hz.\nNext, I will use the data collected by Abur (2014) to examine difference in mean absorbance between sexes. The following SQL query calculates the average absorbance for ‘Abur_2014,’ grouped by frequency and sex of the subjects, while excluding entries where the sex is unknown. It joins two tables, Measurements and Subjects, using a shared subject identifier. The results are grouped by frequency and sex, with the averages computed for each group, and the output is sorted by frequency and sex. This provides a clear breakdown of absorbance patterns by these two attributes.\n\nSELECT \n    m.Frequency, \n    s.Sex, \n    AVG(m.Absorbance) AS mean_absorbance_sex\nFROM \n    Measurements m\nJOIN \n    Subjects s\nON \n    m.SubjectNumber = s.SubjectNumber\nWHERE \n    m.Identifier = 'Abur_2014'\n    AND s.Sex != 'Unknown'\nGROUP BY \n    m.Frequency, s.Sex\nORDER BY \n    m.Frequency, s.Sex;\n\nThis R code creates a plot of mean absorbance by sex in the study by Abur (2014).\n\nlibrary(ggplot2)\n\nggplot(new_table4, aes(x = Frequency, y = mean_absorbance_sex, color = Sex)) +\n  geom_line(size = 0.5, na.rm = TRUE) +\n  scale_x_continuous(\n    name = \"Frequency (Hz)\",\n    breaks = c(200, 1000, 2000, 4000, 6000),\n    limits = c(200, 6000)\n  ) +\n  scale_y_continuous(\n    name = \"Mean Absorbance\",\n    limits = c(0, 1), \n    breaks = seq(0, 1, by = 0.1)\n  ) +\n  labs(\n    title = \"Mean Absorbance by Gender in Abur (2014)\",\n    color = NULL \n  ) +\n  theme_minimal()\n\n\n\n\nAnalyzing the plot, I see that mean absorbance is very similar between sexes. Males have slightly higher mean absorbance from 1500Hz to 3500Hz, where it then swaps to females having slightly higher mean absorbance from 3500Hz to 5000Hz."
  },
  {
    "objectID": "MarchMadness.html",
    "href": "MarchMadness.html",
    "title": "March Madness",
    "section": "",
    "text": "This is analysis of a data set of NCAA Men’s March Madness, which can be found at https://www.kaggle.com/datasets/nishaanamin/march-madness-data.\n\nlibrary(ggplot2)\nteam_results &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-03-26/team-results.csv')\n\nggplot(team_results, aes(y = R64, x = PAKE)) +\n  geom_point(aes(color = CHAMP), size = 3, alpha = 0.8) +\n  scale_color_gradient(low=\"lightblue\", high=\"black\") +\n  labs(\n    y = \"tournaments attended\",\n    x = \"performance against komputer expectation\",\n    title = \"Tournaments Attended Vs. Performance Against Komputer Expectation\"\n)\n\n\n\n\nAnalyzing this plot, it was interesting to see that schools with less than 4 attendances seemed to generally perform better against komputer expectation than those with between 4 and 8 attendances. It is also interesting to note that there are no champions from schools with less than 9 attendances. As expected, championship programs tended to have high performance against komputer expectation scores, with the top 2 scores both coming from multi time champs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alden Kling",
    "section": "",
    "text": "Welcome!"
  }
]